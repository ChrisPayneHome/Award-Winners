---
title: "Award Prediction"
author: "Christian"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: hpstr
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(pROC)

set.seed(442)
```

# Award Prediction

How predictive are other awards in predicting the probability of winning Best Film at the Oscars? For this project we'll be taking data from the big awards that take place prior to the Oscars and 

For this project we'll be using data scraped from the Wikipedia lists of winners for each award:

* [Oscars]("https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films")
* [Golden Globes]("https://en.wikipedia.org/wiki/List_of_Golden_Globe_winners")
* [Palme D'Or]("https://en.wikipedia.org/wiki/Palme_d%27Or#Winners")
* [BAFTAs]("https://en.wikipedia.org/wiki/BAFTA_Award_for_Best_Film")

## Import Data

### Oscar Data

Firstly, we need to import the data on Oscar winners. This dataset contains data on the name of the film, year nominated, awards won, awards nominated, and whether that film won best picture.

```{r import-oscars, message=FALSE, warning=FALSE}

oscar_data <- read_csv("Data/oscar_best_film.csv")

head(oscar_data)
```

### Golden Globes Data

Next, we'll need to import the data on Golden Globes. The data requires a little bit of cleaning before we can use it. Specifically we'll need to extract the names of the film that won, or associated with the winner of, each award.

```{r import-gg, message=FALSE, warning=FALSE}

gg_data <- read_csv("Data/gg_winners.csv") %>%
  mutate(Year = as.numeric(str_sub(Year, 1, 4)),
         Drama_actor = sapply(str_split(`Drama Actor`, ","), `[`, 2),
         Drama_actress = sapply(str_split(`Drama Actress`, ","), `[`, 2),
         Director = sapply(str_split(Director, ","), `[`, 2),
         MC_actor = sapply(str_split(`Musical/Comedy Actor`, ","), `[`, 2),
         MC_actress = sapply(str_split(`Musical/Comedy Actress`, ","), `[`, 2)) %>%
  select(Year,
         Drama_actor,
         Drama_actress,
         Director,
         MC_actor,
         MC_actress)
head(gg_data)
```

### Palme D'Or Winners

After that, we'll need to import the dataset of Palme D'Or winners. This doesn't require any cleaning really, just selecting the only relevant columns.

```{r import-pdo, message=FALSE, warning=FALSE}

pdo_data <- read_csv("Data/pdo_winners.csv") %>%
  select(Year, Film)

```

### Bafta Data

Finally we'll need to import the data for the BAFTAS. This will require a bit of cleaning and concatenation of the datasets for each decade.

```{r import-baftas, message=FALSE, warning=FALSE}

baftas_list <- list.files("Data/BAFTA")

for (data in baftas_list) {
  data <- read_csv(paste0("Data/BAFTA/", data))
  if (exists("bafta_data")) {
    bafta_data <- rbind(bafta_data, data)
  } else {
    bafta_data <- data
  }
}

bafta_data <- bafta_data %>%
  mutate(Year = as.numeric(substring(Year, 1, 4)),
         Film = gsub('[0-9]{2}', '', Film)) %>%
  select(Year, Film, Won) %>%
  arrange(-Year)

bafta_data$Film[16] <- "1917"

head(bafta_data)
```

## Clean Data

Now that we've imported the data, we can now begin to construct the dataset for analysis.

```{r final-data, message=FALSE, warning=FALSE}

final_data <- oscar_data %>%
  
  # join on golden globe data
  left_join(gg_data, by = "Year") %>%
  mutate(gg_drama_actor = ifelse(Film == Drama_actor, TRUE, FALSE),
         gg_drama_actress = ifelse(Film == Drama_actress, TRUE, FALSE),
         gg_director = ifelse(Film == Director, TRUE, FALSE),
         gg_mc_actor = ifelse(Film == MC_actor, TRUE, FALSE),
         gg_mc_actress = ifelse(Film == MC_actress, TRUE, FALSE)) %>%
  select(-Drama_actor,
         -Drama_actress,
         -Director,
         -MC_actor,
         -MC_actress) %>%
  
  # join on Palme D'Or data
  left_join(pdo_data, by = "Year", suffix = c("", "_pdo")) %>%
  mutate(pdo_winner = ifelse(Film == Film_pdo, TRUE, FALSE)) %>%
  select(-Film_pdo) %>%
  
  # join the bafta data
  left_join(bafta_data, by = c("Year", "Film"), suffix = c("", "_bafta")) %>%
  mutate(bafta_nominated = ifelse(!is.na(Won_bafta) & !Won_bafta, TRUE, FALSE),
         bafta_winner = ifelse(!is.na(Won_bafta) & Won_bafta, TRUE, FALSE),
         bafta_not_nominated = ifelse(is.na(Won_bafta), TRUE, FALSE)) %>%
  select(-Won_bafta) %>%
  filter(Year >= 1970) %>%
  replace(is.na(.), FALSE)

```


## Perform analysis

With the data prepared, we can now use it to train the model. Because we're interested in the explainability of the model, I'm selecting a binary logistic regression model.s

```{r train-model1, message=FALSE, warning=FALSE}

index <- createDataPartition(final_data$Won, p = .70, list = FALSE)
train <- final_data[index, ]
test <- final_data[-index, ]


# create binary logiistic model
model <- glm(Won ~ Nominations + 
                    gg_drama_actor +
                    gg_drama_actress +
                    gg_director + 
                    gg_mc_actor + 
                    gg_mc_actress +
                    pdo_winner +
                    bafta_nominated +
                    bafta_winner + 
                    bafta_not_nominated, 
              family = binomial(), 
              data = train)

summary(model)

```

## Evaluate the model

The next step is naturally to evaluate the model using a variety of model evaluation metrics.

### Confusion matrix

This is a simple table comparing the predictions of the model with the actual results from the test data. This will help us visualise the performance of the model. 

```{r cm-model, message=FALSE, warning=FALSE}
pred_prob <- predict(model, test, type = "response")

test$predicted_winner <- ifelse(pred_prob >= 0.5, "Won", "Didn't Win")

confusion_matrix <- table(ifelse(test$Won == TRUE, "Won", "Didn't Win"), test$predicted_winner)
confusion_matrix
```

### Model accuracy

With this table we can calculate the accuracy of the model.

```{r accuracy-model, message=FALSE, warning=FALSE}

Accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix) * 100
Accuracy

```

### Precision

Another model metric we can use is 'precision'. Simply stated, this is the proportion of relevant items that have been retrieved. Formally, it is defined as the number of true positives divided by the number of items the model identified as positives.

```{r precision-model, message=FALSE, warning=FALSE}

Precision <- (confusion_matrix[2, 2] / sum(confusion_matrix[2, ])) * 100
Precision

```

### Recall

Next we'll calculate 'recall'. This is simply the proportion of retrieved items that are relevant.

```{r recall-model, message=FALSE, warning=FALSE}

Recall <- (confusion_matrix[2, 2] / sum(confusion_matrix[, 2])) * 100
Recall

```

### F1 score

With both precision and recall, we can then calculate the mean of the two. This metric is called the F1 score and provides a single metric that provides a balance between the two.

```{r f1-model, message=FALSE, warning=FALSE}

F_Score <- (2 * Precision * Recall / (Precision + Recall)) / 100
F_Score

```

### Plotting ROC curve

The final model metric that we'll calculate is the ROC curve. This plot showcases model performance at different classification thresholds.

```{r roc-model, message=FALSE, warning=FALSE}
predicted <- predict(model, test, type="response")
rocobj <- roc(test$Won, predicted)

ggroc(rocobj) + 
  scale_y_continuous(expand = c(0,0)) + 
  geom_abline(slope = 1, intercept = 1, color = "red", linetype = 2) + 
  labs(x = "Specificity", 
       y = "Sensitivity",
       title = "ROC Curve for Logistic Regression\n") +
  theme_classic()

```


## Create prediction data

With the model trained and evaluated, we can now create the data for the upcoming awards and apply the model to try band predict the winner.

```{r pred-data, message=FALSE, warning=FALSE}
# create data for upcoming ceremony
final_pred_data <- read_csv("Data/oscar_best_film_2022.csv") %>%
  
  # join on golden globe data
  left_join(gg_data, by = "Year") %>%
  mutate(gg_drama_actor = ifelse(Film == Drama_actor, TRUE, FALSE),
         gg_drama_actress = ifelse(Film == Drama_actress, TRUE, FALSE),
         gg_director = ifelse(Film == Director, TRUE, FALSE),
         gg_mc_actor = ifelse(Film == MC_actor, TRUE, FALSE),
         gg_mc_actress = ifelse(Film == MC_actress, TRUE, FALSE)) %>%
  select(-Drama_actor,
         -Drama_actress,
         -Director,
         -MC_actor,
         -MC_actress) %>%
  
  # join on Palme D'Or data
  left_join(pdo_data, by = "Year", suffix = c("", "_pdo")) %>%
  mutate(pdo_winner = ifelse(Film == Film_pdo, TRUE, FALSE)) %>%
  select(-Film_pdo) %>%
  
  # join the bafta data
  left_join(bafta_data, by = c("Year", "Film"), suffix = c("", "_bafta")) %>%
  mutate(bafta_nominated = ifelse(!is.na(Won_bafta) & !Won_bafta, TRUE, FALSE),
         bafta_winner = ifelse(!is.na(Won_bafta) & Won_bafta, TRUE, FALSE),
         bafta_not_nominated = ifelse(is.na(Won_bafta), TRUE, FALSE)) %>%
  select(-Won_bafta)


# predict winner for final data
final_pred_data <- final_pred_data %>%
  mutate(Won = predict(model, ., type = "response")) %>%
  arrange(-Won) %>%
  select(Film, Won)

head(final_pred_data)
```
